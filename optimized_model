import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

#  Check for GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ------------------------------
# 1 **Define the Hybrid Model**
# ------------------------------
class HybridModel(nn.Module):
    def __init__(self, input_dim=5000, d_model=128, num_heads=4, num_layers=2, fcnn_hidden=64):
        super(HybridModel, self).__init__()

        self.input_projection = nn.Linear(input_dim, d_model)  # ✅ Project input_dim -> d_model
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True),
            num_layers=num_layers
        )
        self.fc = nn.Sequential(
            nn.Linear(d_model, fcnn_hidden),
            nn.ReLU(),
            nn.Linear(fcnn_hidden, 2)  #  Output shape (batch_size, 2)
        )

    def forward(self, x):
        x = self.input_projection(x)  # Reduce 5000-dim input to d_model size
        x = self.transformer(x)
        x = self.fc(x[:, -1, :])  #  Use last timestep for prediction
        return x

#  Instantiate the model
model = HybridModel().to(device)
print(" Optimized Hybrid Model Initialized!")

# ------------------------------
# 2 **Create Sample Dataset**
# ------------------------------
batch_size = 4
seq_len = 2
feature_dim = 5000  #  Must match input_dim of the model

#  Generate random dataset (Replace with your real dataset)
X_train = torch.randn(100, seq_len, feature_dim)  # (num_samples, seq_len, feature_dim)
y_train = torch.randn(100, 2)  # (num_samples, 2)

#  Create DataLoader
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

print(f" Dataset Loaded! Train batch shape: {next(iter(train_loader))[0].shape}")

# ------------------------------
# 3️ **Define Training Setup**
# ------------------------------
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

epochs = 20
early_stopping_patience = 3
best_loss = float("inf")
patience_counter = 0

# ------------------------------
# 4️ **Training Loop**
# ------------------------------
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        features, targets = batch  #  Unpack
        features, targets = features.to(device), targets.to(device)  #  Move to device

        optimizer.zero_grad()
        outputs = model(features)

        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
    
        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f" Epoch {epoch+1}/{epochs} - Average Loss: {avg_loss:.6f}")

    #  Early Stopping
    if avg_loss < best_loss:
        best_loss = avg_loss
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= early_stopping_patience:
            print(" Early stopping triggered!")
            break

# ------------------------------
# 5️ **Check Predictions**
# ------------------------------
model.eval()
sample_inputs = torch.randn(10, seq_len, feature_dim).to(device)
sample_outputs = model(sample_inputs).detach().cpu().numpy()
print(f" Sample Predictions: {sample_outputs}")
